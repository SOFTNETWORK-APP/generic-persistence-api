include "softnetwork-clustering.conf"

akka {
  # The recommended way to leave a cluster is a graceful exit, informing the cluster that a node shall leave. This is performed by Coordinated Shutdown when the ActorSystem is terminated and also when a SIGTERM is sent from the environment to stop the JVM process.
  coordinated-shutdown.exit-jvm = on

  actor {
    provider = "cluster"

    debug {
      receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
      autoreceive = off // log all special messages like Kill, PoisonPill etc sent to all actors
      lifecycle = off // log all actor lifecycle events of all actors
      fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
      event-stream = off // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
    }

    # Set this to on to enable serialization-bindings define in
    # additional-serialization-bindings. Those are by default not included
    # for backwards compatibility reasons. They are enabled by default if
    # akka.remote.artery.enabled=on.
    enable-additional-serialization-bindings = on

    allow-java-serialization = off

    serializers {
      proto = "akka.remote.serialization.ProtobufSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      chill  = "com.twitter.chill.akka.AkkaSerializer"
    }

    serialization-bindings {
      "app.softnetwork.persistence.model.package$Timestamped" = proto
      "app.softnetwork.persistence.message.package$ProtobufEvent" = proto # protobuf events
      "app.softnetwork.persistence.model.package$ProtobufDomainObject" = proto # protobuf domain objects
      "app.softnetwork.persistence.model.package$ProtobufStateObject" = proto # protobuf state objects

      "app.softnetwork.persistence.message.package$CborEvent" = jackson-cbor # cbor events
      "app.softnetwork.persistence.model.package$CborDomainObject" = jackson-cbor # cbor domain objects

      "app.softnetwork.persistence.message.package$Command" = chill
      "app.softnetwork.persistence.message.package$CommandResult" = chill
      "app.softnetwork.persistence.message.package$Event" = chill
      "app.softnetwork.persistence.model.package$State" = chill

    }

  }

  persistence {
    # When starting many persistent actors at the same time the journal
    # and its data store is protected from being overloaded by limiting number
    # of recoveries that can be in progress at the same time. When
    # exceeding the limit the actors will wait until other recoveries have
    # been completed.
    max-concurrent-recoveries = 50
    journal {
      plugin = ${softnetwork.persistence.journal.plugin}
      // Enable the line below to automatically start the journal when the actorsystem is started
      // auto-start-journals = [${softnetwork.persistence.journal.plugin}]
    }
    snapshot-store {
      plugin = ${softnetwork.persistence.snapshot-store.plugin}
      // Enable the line below to automatically start the snapshot-store when the actorsystem is started
      // auto-start-snapshot-stores = [${softnetwork.persistence.snapshot-store.plugin}]
    }
    read-journal {
      plugin = ${softnetwork.persistence.read-journal.plugin}
    }

  }

  # By default, just bind to loopback and do not allow access from the network
  remote {
    artery {
      canonical.hostname = ${clustering.ip} # external (logical) hostname
      canonical.port = ${clustering.port} # external (logical) port
    }
  }

  cluster {
    seed-nodes = []

    roles = []

    # to start actors after the cluster has been initialized, members have joined, and the cluster has reached
    # a certain size
    # Itâ€™s recommended to use Cluster Sharding with the Cluster setting akka.cluster.min-nr-of-members
    # or akka.cluster.role.<role-name>.min-nr-of-members.
    # min-nr-of-members will defer the allocation of the shards until at least that number of regions have been started
    # and registered to the coordinator. This avoids that many shards are allocated to the first region that registers
    # and only later are rebalanced to other nodes.
    # min-nr-of-members = 1

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    distributed-data {
      # All entries will be made durable
      durable.keys = ["*"]
      # Directory of LMDB (the default storage implementation for distributed data) file
      durable.lmdb.dir = "/tmp/ddata"
    }

    sharding{
      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = ""

      # Set this to a time duration to have sharding passivate entities when they have not
      # received any message in this length of time. Set to 'off' to disable.
      # It is always disabled if `remember-entities` is enabled.
      passivate-idle-entity-after = 120s

      # Number of shards used by the default HashCodeMessageExtractor
      # when no other message extractor is defined. This value must be
      # the same for all nodes in the cluster and that is verified by
      # configuration check when joining. Changing the value requires
      # stopping all nodes in the cluster.
      # the number of shards should be a factor ten greater than the planned maximum number of cluster nodes
      # number-of-shards = 1000

      # When a node is added to the cluster the shards on the existing nodes will be rebalanced to the new node.
      # The LeastShardAllocationStrategy picks shards for rebalancing from the ShardRegions with most number of
      # previously allocated shards. They will then be allocated to the ShardRegion with least number of
      # previously allocated shards, i.e. new members in the cluster.
      # The amount of shards to rebalance in each round can be limited to make it progress slower
      # since rebalancing too many shards at the same time could result in additional load on the system.
      least-shard-allocation-strategy{
        # The rebalance-absolute-limit is the maximum number of shards that will be rebalanced in one rebalance round.
        rebalance-absolute-limit = 10
        # The rebalance-relative-limit is a fraction (< 1.0) of total number of (known) shards that
        # will be rebalanced in one rebalance round
        rebalance-relative-limit = 0.1
      }

      # A state store is mandatory for sharding, it contains the location of shards.
      state-store-mode = ddata # the default

      # Set this to a time duration to have sharding passivate entities when they have not
      # received any message in this length of time. Set to 'off' to disable.
      # It is always disabled if `remember-entities` is enabled.
      passivate-idle-entity-after = 120s

      # Remembering entities automatically restarts entities after a rebalance or entity crash.
      # Without remembered entities restarts happen on the arrival of a message.
      # Enabling remembered entities disables Automatic Passivation.
      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # When 'remember-entities' is enabled and the state store mode is ddata this controls
      # how the remembered entities and shards are stored. Possible values are "eventsourced" and "ddata"
      # Default is ddata for backwards compatibility.
      remember-entities-store = eventsourced

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence and/or remember-entities-store = eventsourced
      journal-plugin-id = ${softnetwork.persistence.journal.plugin}

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence and/or remember-entities-store = eventsourced
      snapshot-plugin-id = ${softnetwork.persistence.snapshot-store.plugin}
    }
  }

}
