akka {
  # The recommended way to leave a cluster is a graceful exit, informing the cluster that a node shall leave. This is performed by Coordinated Shutdown when the ActorSystem is terminated and also when a SIGTERM is sent from the environment to stop the JVM process.
  coordinated-shutdown.exit-jvm = on

  actor {
    provider = "cluster"

    debug {
      receive = on // log all messages sent to an actor if that actors receive method is a LoggingReceive
      autoreceive = off // log all special messages like Kill, PoisonPill etc sent to all actors
      lifecycle = off // log all actor lifecycle events of all actors
      fsm = off // enable logging of all events, transitioffs and timers of FSM Actors that extend LoggingFSM
      event-stream = off // enable logging of subscriptions (subscribe/unsubscribe) on the ActorSystem.eventStream
    }

    # Set this to on to enable serialization-bindings define in
    # additional-serialization-bindings. Those are by default not included
    # for backwards compatibility reasons. They are enabled by default if
    # akka.remote.artery.enabled=on.
    enable-additional-serialization-bindings = on

    allow-java-serialization = off

    serializers {
      proto = "akka.remote.serialization.ProtobufSerializer"
      jackson-cbor = "akka.serialization.jackson.JacksonCborSerializer"
      chill  = "com.twitter.chill.akka.AkkaSerializer"
    }

    serialization-bindings {
      "app.softnetwork.persistence.model.package$Timestamped" = proto
      "app.softnetwork.persistence.message.package$ProtobufEvent" = proto # protobuf events
      "app.softnetwork.persistence.model.package$ProtobufDomainObject" = proto # protobuf domain objects
      "app.softnetwork.persistence.model.package$ProtobufStateObject" = proto # protobuf state objects

      "app.softnetwork.persistence.message.package$CborEvent" = jackson-cbor # cbor events
      "app.softnetwork.persistence.model.package$CborDomainObject" = jackson-cbor # cbor domain objects

      "app.softnetwork.persistence.message.package$Command" = chill
      "app.softnetwork.persistence.message.package$CommandResult" = chill
      "app.softnetwork.persistence.message.package$Event" = chill
      "app.softnetwork.persistence.model.package$State" = chill

    }

  }

  persistence {
    # When starting many persistent actors at the same time the journal
    # and its data store is protected from being overloaded by limiting number
    # of recoveries that can be in progress at the same time. When
    # exceeding the limit the actors will wait until other recoveries have
    # been completed.
    max-concurrent-recoveries = 50
    journal {
      plugin = ${softnetwork.persistence.journal.plugin}
      // Enable the line below to automatically start the journal when the actorsystem is started
      // auto-start-journals = [${softnetwork.persistence.journal.plugin}]
    }
    snapshot-store {
      plugin = ${softnetwork.persistence.snapshot-store.plugin}
      // Enable the line below to automatically start the snapshot-store when the actorsystem is started
      // auto-start-snapshot-stores = [${softnetwork.persistence.snapshot-store.plugin}]
    }
    read-journal {
      plugin = ${softnetwork.persistence.read-journal.plugin}
    }

  }

  # By default, just bind to loopback and do not allow access from the network
  remote {
    artery {
      canonical {
        # Hostname clients should connect to. Can be set to an ip, hostname
        # or one of the following special values:
        #   "<getHostAddress>"   InetAddress.getLocalHost.getHostAddress
        #   "<getHostName>"      InetAddress.getLocalHost.getHostName
        hostname = "<getHostAddress>" # external (logical) hostname
        hostname = ${?CLUSTER_IP}
        # The default remote server port clients should connect to.
        # Default is 25520, use 0 if you want a random available port
        # This port needs to be unique for each actor system on the same machine.
        port = 25520 # external (logical) port
        port = ${?CLUSTER_PORT}
      }
    }
  }

  cluster {
    seed-nodes = []

    roles = []

    # to start actors after the cluster has been initialized, members have joined, and the cluster has reached
    # a certain size
    # Itâ€™s recommended to use Cluster Sharding with the Cluster setting akka.cluster.min-nr-of-members
    # or akka.cluster.role.<role-name>.min-nr-of-members.
    # min-nr-of-members will defer the allocation of the shards until at least that number of regions have been started
    # and registered to the coordinator. This avoids that many shards are allocated to the first region that registers
    # and only later are rebalanced to other nodes.
    # min-nr-of-members = 1

    downing-provider-class = "akka.cluster.sbr.SplitBrainResolverProvider"

    distributed-data {
      # All entries will be made durable
      durable.keys = ["*"]
      # Directory of LMDB (the default storage implementation for distributed data) file
      durable.lmdb.dir = "/tmp/ddata"
    }

    sharding{
      # Specifies that entities runs on cluster nodes with a specific role.
      # If the role is not specified (or empty) all nodes in the cluster are used.
      role = ""

      # Set this to a time duration to have sharding passivate entities when they have not
      # received any message in this length of time. Set to 'off' to disable.
      # It is always disabled if `remember-entities` is enabled.
      passivate-idle-entity-after = 120s

      # Number of shards used by the default HashCodeMessageExtractor
      # when no other message extractor is defined. This value must be
      # the same for all nodes in the cluster and that is verified by
      # configuration check when joining. Changing the value requires
      # stopping all nodes in the cluster.
      # the number of shards should be a factor ten greater than the planned maximum number of cluster nodes
      # number-of-shards = 1000

      # When a node is added to the cluster the shards on the existing nodes will be rebalanced to the new node.
      # The LeastShardAllocationStrategy picks shards for rebalancing from the ShardRegions with most number of
      # previously allocated shards. They will then be allocated to the ShardRegion with least number of
      # previously allocated shards, i.e. new members in the cluster.
      # The amount of shards to rebalance in each round can be limited to make it progress slower
      # since rebalancing too many shards at the same time could result in additional load on the system.
      least-shard-allocation-strategy{
        # The rebalance-absolute-limit is the maximum number of shards that will be rebalanced in one rebalance round.
        rebalance-absolute-limit = 10
        # The rebalance-relative-limit is a fraction (< 1.0) of total number of (known) shards that
        # will be rebalanced in one rebalance round
        rebalance-relative-limit = 0.1
      }

      # A state store is mandatory for sharding, it contains the location of shards.
      state-store-mode = ddata # the default

      # Set this to a time duration to have sharding passivate entities when they have not
      # received any message in this length of time. Set to 'off' to disable.
      # It is always disabled if `remember-entities` is enabled.
      passivate-idle-entity-after = 120s

      # Remembering entities automatically restarts entities after a rebalance or entity crash.
      # Without remembered entities restarts happen on the arrival of a message.
      # Enabling remembered entities disables Automatic Passivation.
      # When this is set to 'on' the active entity actors will automatically be restarted
      # upon Shard restart. i.e. if the Shard is started on a different ShardRegion
      # due to rebalance or crash.
      remember-entities = off

      # When 'remember-entities' is enabled and the state store mode is ddata this controls
      # how the remembered entities and shards are stored. Possible values are "eventsourced" and "ddata"
      # Default is ddata for backwards compatibility.
      remember-entities-store = eventsourced

      # Absolute path to the journal plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default journal plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence and/or remember-entities-store = eventsourced
      journal-plugin-id = ${softnetwork.persistence.journal.plugin}

      # Absolute path to the snapshot plugin configuration entity that is to be
      # used for the internal persistence of ClusterSharding. If not defined
      # the default snapshot plugin is used. Note that this is not related to
      # persistence used by the entity actors.
      # Only used when state-store-mode=persistence and/or remember-entities-store = eventsourced
      snapshot-plugin-id = ${softnetwork.persistence.snapshot-store.plugin}
    }
  }

  discovery{
    method = aggregate
    aggregate {
      # List of service discovery methods to try in order. E.g config then fall back to DNS
      # ["config", "akka-dns"]
      discovery-methods = ["akka-dns", "config"]
    }
    config.services = {
      local = {
        endpoints = [
          {
            host = "127.0.0.1"
            host = ${?CLUSTER_MANAGEMENT_HOST}
            port = 8558
            port = ${?CLUSTER_MANAGEMENT_PORT}
          }
        ]
      }
    }
  }

  management {
    http {
      # The hostname where the HTTP Server for Http Cluster Management will be started.
      # This defines the interface to use.
      # InetAddress.getLocalHost.getHostAddress is used not overriden or empty
      hostname = ""
      hostname = ${?CLUSTER_MANAGEMENT_HOST}

      # The port where the HTTP Server for Http Cluster Management will be bound.
      # The value will need to be from 0 to 65535.
      port = 8558 # port pun, it "complements" 2552 which is often used for Akka remoting
      port = ${?CLUSTER_MANAGEMENT_PORT}

      # Use this setting to bind a network interface to a different hostname or ip
      # than the HTTP Server for Http Cluster Management.
      # Use "0.0.0.0" to bind to all interfaces.
      # akka.management.http.hostname if empty
      bind-hostname = ""

      # Use this setting to bind a network interface to a different port
      # than the HTTP Server for Http Cluster Management. This may be used
      # when running akka nodes in a separated networks (under NATs or docker containers).
      # Use 0 if you want a random available port.
      #
      # akka.management.http.port if empty
      bind-port = ""

      # path prefix for all management routes, usually best to keep the default value here. If
      # specified, you'll want to use the same value for all nodes that use akka management so
      # that they can know which path to access each other on.
      base-path = ""

      routes {
        health-checks = "akka.management.HealthCheckRoutes"
      }

      # Should Management route providers only expose read only endpoints? It is up to each route provider
      # to adhere to this property
      route-providers-read-only = true
    }

    # Health checks for readiness and liveness
    health-checks {
      # When exposting health checks via Akka Management, the path to expost readiness checks on
      readiness-path = "ready"
      # When exposting health checks via Akka Management, the path to expost readiness checks on
      liveness-path = "alive"
      # All readiness checks are executed in parallel and given this long before the check is timed out
      check-timeout = 1s
    }

    cluster.bootstrap {
      contact-point-discovery {
        discovery-method = akka.discovery
        discovery-method = ${?CONTACT_POINT_DISCOVERY_METHOD}
        required-contact-point-nr = 1
        required-contact-point-nr = ${?REQUIRED_CONTACT_POINT_NR}
        service-name = local
        service-name = ${?CONTACT_POINT_SERVICE_NAME} #AKKA_CLUSTER_BOOTSTRAP_SERVICE_NAME
        # Added as suffix to the service-name to build the effective-service name used in the contact-point service lookups
        # If undefined, nothing will be appended to the service-name.
        #
        # Examples, set this to:
        # "default.svc.cluster.local" or "my-namespace.svc.cluster.local" for kubernetes clusters.
        service-namespace = "<service-namespace>"
        service-namespace = ${?CONTACT_POINT_SERVICE_NAMESPACE}
      }
    }
  }
}

clustering {
  cluster.name = Softnetwork
  cluster.name = ${?CLUSTER_NAME}
}
